{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AdhigL5wk12E",
        "xSFTT7hIKMfB",
        "PZqxpMGfKSVN",
        "-fmvDZhCM78x"
      ],
      "authorship_tag": "ABX9TyP8EsaVPgNH+k+6rc2gJlbX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pc-Jiang/math_in_deep_learning/blob/main/DSC_291_SP23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DSC 291 Mathmatics in Deep Learning, SP23\n",
        "Advisor: Mikhail Belkin\n",
        "\n",
        "Author: Pengcen Jiang, Bin Wang"
      ],
      "metadata": {
        "id": "Wu1mT0KsGSct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages and set random seed.\n",
        "\n"
      ],
      "metadata": {
        "id": "AdhigL5wk12E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NBeePU9YZl3r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms       \n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "MAP_LOC = \"cuda:0\" if USE_CUDA else torch.device('cpu')\n",
        "\n",
        "def set_seed(seed = 1234):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define basic blocks and functions. "
      ],
      "metadata": {
        "id": "jeXs-RtvkyjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define configurations class. "
      ],
      "metadata": {
        "id": "xSFTT7hIKMfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnFixedPointConfig(object):\n",
        "    def __init__(self):\n",
        "        # params for model\n",
        "        self.hidden_size = 100\n",
        "        self.nonlinearity = None\n",
        "        self.dim_in = 10\n",
        "        self.dim_out = 10\n",
        "        self.batch_size = 1\n",
        "        self.initialization = None\n",
        "        self.use_bias = True\n",
        "        self.time_steps = 10\n",
        "        self.ct = False\n",
        "        self.tau = 0\n",
        "\n",
        "        # params for training\n",
        "        self.lr = 0.001\n",
        "        self.optimizer_type = 'SGD'\n",
        "        self.momentum = 0\n",
        "        self.wdecay = 0\n",
        "        self.num_ep = 100\n",
        "        self.add_noise = False\n",
        "\n",
        "        # params for generating data\n",
        "        self.std_fp = 1\n",
        "        self.mean_fp = 0\n",
        "        \n",
        "\n",
        "    def update(self, new_config):\n",
        "        self.__dict__.update(new_config.__dict__)\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.__dict__)"
      ],
      "metadata": {
        "id": "IRsB6j4zJD1Y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define RNN class. "
      ],
      "metadata": {
        "id": "PZqxpMGfKSVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IdentityAct(object):\n",
        "    def __call__(self, tensor):\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class VanillaRNNCell(nn.Module):\n",
        "    # (N, L, H_{in}): N, batch size, L, squence length, H_{in}, input size\n",
        "    def __init__(self, config):\n",
        "        super(VanillaRNNCell, self).__init__()\n",
        "        self.input_size = config.dim_in\n",
        "        self.output_size = config.dim_out\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.nonlinearity = config.nonlinearity\n",
        "        self.use_bias = config.use_bias\n",
        "        self.initialization = config.initialization\n",
        "        self.ct = config.ct\n",
        "        self.tau = config.tau\n",
        "\n",
        "        self.weight_ih = nn.Parameter(\n",
        "            torch.zeros((self.hidden_size, self.input_size)))\n",
        "        self.weight_hh = nn.Parameter(\n",
        "            torch.zeros((self.hidden_size, self.hidden_size)))\n",
        "        self.weight_ho = nn.Parameter(\n",
        "            torch.zeros((self.output_size, self.hidden_size)))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.hidden_size, 1))\n",
        "        self.bias_out = nn.Parameter(torch.zeros(self.output_size, 1))\n",
        "        self.reset_parameters()\n",
        "\n",
        "        if self.nonlinearity is None:\n",
        "            self.act = IdentityAct()\n",
        "        elif self.nonlinearity == \"tanh\":\n",
        "            self.act = torch.tanh\n",
        "        elif self.nonlinearity == \"relu\":\n",
        "            self.act = F.relu\n",
        "        else:\n",
        "            raise RuntimeError(\"Unknown nonlinearity: {}\".format(\n",
        "                self.nonlinearity))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # add: bias-false\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        \n",
        "        if self.initialization is None:\n",
        "            for name, weight in self.named_parameters():\n",
        "              if 'bias' in name:\n",
        "                  if not self.use_bias:\n",
        "                      weight.requires_grad = False\n",
        "        elif self.initialization == 'uniform':\n",
        "            for name, weight in self.named_parameters():\n",
        "                if 'bias' in name:\n",
        "                    if not self.use_bias:\n",
        "                        weight.requires_grad = False\n",
        "                    else:\n",
        "                        nn.init.uniform_(weight, -stdv, stdv)\n",
        "                else:\n",
        "                    nn.init.uniform_(weight, -stdv, stdv)\n",
        "        else:\n",
        "            raise RuntimeError(\"Unknown initialization: {}\".format(self.initialization)) \n",
        "\n",
        "    def forward(self, inp, hidden_in):\n",
        "        inp = torch.unsqueeze(inp, 2)\n",
        "        hidden_in = torch.unsqueeze(hidden_in, 2)\n",
        "        if not self.ct:\n",
        "            hidden_out = self.act(\n",
        "                torch.matmul(self.weight_ih, inp) +\n",
        "                torch.matmul( self.weight_hh, hidden_in) + self.bias)\n",
        "        else:\n",
        "            alpha = 1 / self.tau\n",
        "            hidden_out = (1 - alpha) * hidden_in \\\n",
        "                         + alpha * self.act(torch.matmul(self.weight_ih, inp)\n",
        "                                            + torch.matmul(self.weight_hh, hidden_in)\n",
        "                                            + self.bias\n",
        "                                            )\n",
        "\n",
        "        output = torch.matmul(self.weight_ho, hidden_out) + self.bias_out\n",
        "        hidden_out = torch.squeeze(hidden_out, 2)\n",
        "        output = torch.squeeze(output, 2)\n",
        "\n",
        "        return [hidden_out, output]\n",
        "\n",
        "    def init_hidden(self, batch_s):\n",
        "        return torch.zeros(batch_s, self.hidden_size).to(DEVICE)"
      ],
      "metadata": {
        "id": "glHb1YSTZrRc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the function to get the fixed point. "
      ],
      "metadata": {
        "id": "-fmvDZhCM78x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fixed_point(config):\n",
        "    dim = config.dim_in\n",
        "    fixed_point = torch.randn(1, dim).to(DEVICE) * config.std_fp + config.mean_fp\n",
        "    return fixed_point\n",
        "\n",
        "def add_noise(fixed_point, mean=0, std=0.1):\n",
        "    return fixed_point + torch.randn(fixed_point.size()).to(DEVICE) * std + mean"
      ],
      "metadata": {
        "id": "CXOgBFymM7P7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the training function"
      ],
      "metadata": {
        "id": "SgJwmbH4HFvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fixed_point(config, model, fixed_point):\n",
        "    # initialize optimizer\n",
        "    if config.optimizer_type == 'Adam':\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,\n",
        "                        model.parameters()),\n",
        "                        lr=config.lr,\n",
        "                        weight_decay=config.wdecay)\n",
        "    elif config.optimizer_type == 'SGD':\n",
        "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad,\n",
        "                       model.parameters()),\n",
        "                       lr=config.lr,\n",
        "                       momentum=config.momentum,\n",
        "                       weight_decay=config.wdecay)\n",
        "    else:\n",
        "        raise NotImplementedError('optimizer not implemented')\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    task_loss_list = []\n",
        "    ep_list = []\n",
        "\n",
        "    count = 1\n",
        "    for ep in range(config.num_ep):\n",
        "        hidden = model.init_hidden(config.batch_size)\n",
        "        model.train()\n",
        "        loss = 0.0 \n",
        "        optimizer.zero_grad()\n",
        "        for step in range(config.time_steps):\n",
        "            \n",
        "            if config.add_noise:\n",
        "                fixed_point = add_noise(fixed_point)\n",
        "            [hidden, output] = model(fixed_point, hidden)\n",
        "            task_loss = criterion(output, fixed_point)\n",
        "            loss += task_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        task_loss_list.append((loss/config.time_steps).item())\n",
        "        ep_list.append(count)\n",
        "        if count % 10 == 0: \n",
        "            print('TRAIN | Epoch: {}/{} | Loss: {:.2f}'.format(ep+1, config.num_ep, loss/config.time_steps))\n",
        "        count += 1\n",
        "\n",
        "    return model, optimizer, task_loss_list, ep_list\n"
      ],
      "metadata": {
        "id": "p6IXhz5IG0cE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the network. "
      ],
      "metadata": {
        "id": "s7v1U5Akk8eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LearnFixedPointConfig()\n",
        "model = VanillaRNNCell(config)\n",
        "fixed_point = get_fixed_point(config)\n",
        "print('Fixed point {} \\n'.format(fixed_point))"
      ],
      "metadata": {
        "id": "srbeDvEKHa6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d745d3-e66c-4bfd-e245-822e61428ffc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed point tensor([[ 0.0461,  0.4024, -1.0115,  0.2167, -0.6123,  0.5036,  0.2310,  0.6931,\n",
            "         -0.2669,  2.1785]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer, task_loss_list, ep_list = train_fixed_point(config, model, fixed_point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeE5NakzYT23",
        "outputId": "15d8e84c-8081-42cb-9b0f-aab1303ae3c6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN | Epoch: 10/100 | Loss: 0.44\n",
            "TRAIN | Epoch: 20/100 | Loss: 0.43\n",
            "TRAIN | Epoch: 30/100 | Loss: 0.41\n",
            "TRAIN | Epoch: 40/100 | Loss: 0.39\n",
            "TRAIN | Epoch: 50/100 | Loss: 0.38\n",
            "TRAIN | Epoch: 60/100 | Loss: 0.36\n",
            "TRAIN | Epoch: 70/100 | Loss: 0.35\n",
            "TRAIN | Epoch: 80/100 | Loss: 0.33\n",
            "TRAIN | Epoch: 90/100 | Loss: 0.32\n",
            "TRAIN | Epoch: 100/100 | Loss: 0.31\n"
          ]
        }
      ]
    }
  ]
}